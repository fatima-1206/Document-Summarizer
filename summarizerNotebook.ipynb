{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "143bfbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fati1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c211c12",
   "metadata": {},
   "source": [
    "## Document Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c5a9c",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b626fa",
   "metadata": {},
   "source": [
    "File Path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76203ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './problemStatement.md'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a0f4b",
   "metadata": {},
   "source": [
    "Get the file extension and parse it accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7922e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file extension\n",
    "def get_file_extension(file_path):\n",
    "    return os.path.splitext(file_path)[-1].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80804ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the appropriate text parser based on file extension (pdf, markdown, txt)\n",
    "def parse_file(file_path):\n",
    "    file_extension = get_file_extension(file_path)\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = ''\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + '\\n'\n",
    "            return text\n",
    "    elif file_extension == '.md' or file_extension == '.markdown':\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            html = markdown.markdown(file.read())\n",
    "        return BeautifulSoup(html, \"html.parser\").get_text()\n",
    "    elif file_extension == '.txt':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b58f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = parse_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec419c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Title: Document Summarization using Retrieval-Augmented\n",
      "Generation (RAG)\n",
      "Objective:\n",
      "To develop a summarization system that combines retrieval-based context\n",
      "selection with large language model (LLM) generation. The system should\n",
      "accept a long document and generate a concise, coherent summary using\n",
      "semantic chunking and RAG.\n",
      "\n",
      "Project Tasks:\n",
      "1. Document Ingestion\n",
      "● Accept documents in PDF, TXT, or Markdown format.\n",
      "● Split into semantically meaningful chunks using sliding windows or\n",
      "semantic segmenters.\n",
      "2. Embedding & Retrieval\n",
      "● Convert chunks to vector embeddings using SentenceTransformers or\n",
      "OpenAI API. ● Store in FAISS or Chroma vector DB.\n",
      "● Perform semantic retrieval for a general summary query (e.g.,\n",
      "\"Summarize this document\").\n",
      "3. Summary Generation\n",
      "● Use top-k retrieved chunks and pass them into a pre-trained LLM\n",
      "(e.g., GPT, LLaMA, Mistral).\n",
      "● Generate a final summary that is coherent, fluent, and accurate.\n",
      "4. Output Presentation\n",
      "● Display the retrieved context and the generated summary. ●\n",
      "Optionally show token usage, latency, and similarity scores.\n",
      "Dataset Suggestions:\n",
      "Dataset\n",
      "ArXiv Abstracts\n",
      "CNN/DailyMail\n",
      "Description\n",
      "\n",
      "Scientific article summaries\n",
      "News article + summary\n",
      "pairs\n",
      "\n",
      "Link\n",
      "\n",
      "https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
      "https://huggingface.co/datasets/cnn_dailymail\n",
      "\n",
      "Custom PDFs Local/public documents\n",
      "for summarization\n",
      "\n",
      "Use your own documents or publicly available\n",
      "datasets\n",
      "\n",
      "Tutorials & Guides\n",
      "● HuggingFace RAG Tutorial:\n",
      "https://huggingface.co/blog/rag\n",
      "● LangChain Quickstart:\n",
      "https://docs.langchain.com/docs/get_started/introduction\n",
      "● FAISS Vector Store Tutorial:\n",
      "https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
      "Research Papers & Articles\n",
      "● Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "(Lewis et al., 2020)\n",
      "● Don't Pay Attention: Pay Enough! RAG revisited (Borgeaud et al.,\n",
      "2022) ● ColBERT: Efficient and Effective Passage Retrieval\n",
      "Tools & Libraries\n",
      "● LangChain:\n",
      "https://github.com/hwchase17/langchain\n",
      "● FAISS:\n",
      "https://github.com/facebookresearch/faiss\n",
      "● ChromaDB:\n",
      "https://www.trychroma.com/\n",
      "● SentenceTransformers:\n",
      "https://www.sbert.net/\n",
      "\n",
      "Note to Interns:\n",
      "This is a simplified project focusing on the fundamentals of\n",
      "summarization using pre-trained models and vector search. You are\n",
      "encouraged to be creative and modular in design. We will evaluate your\n",
      "effort and understanding, not just the final output.\n",
      "\n",
      "Submission: ZIP file containing code, a PDF report, and sample\n",
      "results\n",
      "Grading Rubric (100 Points):\n",
      "Category\n",
      "\n",
      "Document Parsing\n",
      "Embedding & Storage\n",
      "Retrieval Quality\n",
      "Summary Generation\n",
      "Pipeline Design\n",
      "Output Presentation\n",
      "Documentation\n",
      "\n",
      "Max Points\n",
      "\n",
      "15\n",
      "15\n",
      "20\n",
      "20\n",
      "10\n",
      "10\n",
      "10\n",
      "\n",
      "Evaluation Criteria\n",
      "\n",
      "Clean loading, chunking, and formatting\n",
      "Efficient use of vector DB and embeddings\n",
      "Relevance of selected content to the document's core idea\n",
      "Fluency, coverage, and accuracy of the summary\n",
      "Clear, modular, and reproducible code\n",
      "Display of retrieved content and generated results\n",
      "ReadMe clarity, report explanation,and visual aids\n",
      "\n",
      "Submission Requirements:\n",
      "● Python code with requirements.txt or environment.yml\n",
      "● ReadMe with setup and usage guide\n",
      "● Sample summarization runs for at least 3 different documents\n",
      "● PDF report (2 pages max)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded9262",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b720e64",
   "metadata": {},
   "source": [
    "Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d4786e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Normalize line breaks and spaces\n",
    "    text = re.sub(r'\\r\\n|\\r', '\\n', text)           # Convert \\r\\n or \\r to \\n\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)          # Collapse many newlines into 2\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)             # Remove extra spaces/tabs\n",
    "\n",
    "    # Normalize unicode \n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "    def add_period_to_bullet(match):\n",
    "        line = match.group(0).strip()\n",
    "        if not line.endswith('.'):\n",
    "            return line + '.'\n",
    "        return line\n",
    "\n",
    "    # Add periods to lines that start with bullet markers (before removing markers)\n",
    "    text = re.sub(r'(?m)^\\s*[-*+]\\s+(.*)', lambda m: \"- \" + add_period_to_bullet(m), text)\n",
    "    # Remove common bullet points\n",
    "    text = re.sub(\n",
    "        r'[\\u2022\\u2023\\u25E6\\u2043\\u2219\\u25AA\\u25AB\\u25CB\\u25CF\\u25A0\\u25B8\\u29BE\\u29BF]',\n",
    "          '', text)\n",
    "\n",
    "    # Remove markdown or ASCII-style tables\n",
    "    text = re.sub(r'\\|.*?\\|', '', text)      # Remove markdown tables\n",
    "    text = re.sub(r'[-=]{3,}', '', text)     # Remove underlines in tables\n",
    "    text = re.sub(r'^\\s*[\\-\\*+]\\s+', '', text, flags=re.MULTILINE)  # Bulleted list lines\n",
    "\n",
    "    # Remove figure/table/image captions\n",
    "    text = re.sub(r'(Figure|Table|Image|Chart|Diagram)\\s*\\d+[\\.:]?', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove bracketed footnotes like [1], [12], (Fig. 3), etc.\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?fig.*?\\)', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Fix line breaks and hyphens split across lines\n",
    "    text = re.sub(r'-\\n', '', text)  # Remove hyphenated line-breaks\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Collapse newlines\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Normalize spaces\n",
    "\n",
    "    # Strip remaining non-ASCII or odd symbols\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # before every \\n add a period if it doesn't end with one special character\n",
    "    text = re.sub(r'(?<![.!?:])\\n', '. \\n', text)     \n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a429201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Title: Document Summarization using Retrieval-Augmented. \n",
      "Generation (RAG). \n",
      "Objective:\n",
      "To develop a summarization system that combines retrieval-based context. \n",
      "selection with large language model (LLM) generation. The system should. \n",
      "accept a long document and generate a concise, coherent summary using. \n",
      "semantic chunking and RAG.\n",
      "Project Tasks:\n",
      "1. Document Ingestion. \n",
      " Accept documents in PDF, TXT, or Markdown format.\n",
      " Split into semantically meaningful chunks using sliding windows or. \n",
      "semantic segmenters.\n",
      "2. Embedding & Retrieval. \n",
      " Convert chunks to vector embeddings using SentenceTransformers or. \n",
      "OpenAI API. Store in FAISS or Chroma vector DB.\n",
      " Perform semantic retrieval for a general summary query (e.g.,. \n",
      "\"Summarize this document\").\n",
      "3. Summary Generation. \n",
      " Use top-k retrieved chunks and pass them into a pre-trained LLM. \n",
      "(e.g., GPT, LLaMA, Mistral).\n",
      " Generate a final summary that is coherent, fluent, and accurate.\n",
      "4. Output Presentation. \n",
      " Display the retrieved context and the generated summary. . \n",
      "Optionally show token usage, latency, and similarity scores.\n",
      "Dataset Suggestions:\n",
      "Dataset. \n",
      "ArXiv Abstracts. \n",
      "CNN/DailyMail. \n",
      "Description. \n",
      "Scientific article summaries. \n",
      "News article + summary. \n",
      "pairs. \n",
      "Link. \n",
      "Custom PDFs Local/public documents. \n",
      "for summarization. \n",
      "Use your own documents or publicly available. \n",
      "datasets. \n",
      "Tutorials & Guides. \n",
      " HuggingFace RAG Tutorial:\n",
      " LangChain Quickstart:\n",
      " FAISS Vector Store Tutorial:\n",
      "Research Papers & Articles. \n",
      " Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. \n",
      "(Lewis et al., 2020). \n",
      " Don't Pay Attention: Pay Enough! RAG revisited (Borgeaud et al.,. \n",
      "2022) ColBERT: Efficient and Effective Passage Retrieval. \n",
      "Tools & Libraries. \n",
      " LangChain:\n",
      " FAISS:\n",
      " ChromaDB:\n",
      " SentenceTransformers:\n",
      "Note to Interns:\n",
      "This is a simplified project focusing on the fundamentals of. \n",
      "summarization using pre-trained models and vector search. You are. \n",
      "encouraged to be creative and modular in design. We will evaluate your. \n",
      "effort and understanding, not just the final output.\n",
      "Submission: ZIP file containing code, a PDF report, and sample. \n",
      "results. \n",
      "Grading Rubric (100 Points):\n",
      "Category. \n",
      "Document Parsing. \n",
      "Embedding & Storage. \n",
      "Retrieval Quality. \n",
      "Summary Generation. \n",
      "Pipeline Design. \n",
      "Output Presentation. \n",
      "Documentation. \n",
      "Max Points. \n",
      "15. \n",
      "15. \n",
      "20. \n",
      "20. \n",
      "10. \n",
      "10. \n",
      "10. \n",
      "Evaluation Criteria. \n",
      "Clean loading, chunking, and formatting. \n",
      "Efficient use of vector DB and embeddings. \n",
      "Relevance of selected content to the document's core idea. \n",
      "Fluency, coverage, and accuracy of the summary. \n",
      "Clear, modular, and reproducible code. \n",
      "Display of retrieved content and generated results. \n",
      "ReadMe clarity, report explanation,and visual aids. \n",
      "Submission Requirements:\n",
      " Python code with requirements.txt or environment.yml. \n",
      " ReadMe with setup and usage guide. \n",
      " Sample summarization runs for at least 3 different documents. \n",
      " PDF report (2 pages max).\n"
     ]
    }
   ],
   "source": [
    "text = clean_text(raw_text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058adda6",
   "metadata": {},
   "source": [
    "## Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78cb07c",
   "metadata": {},
   "source": [
    "Download the embedding model from hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ea0e617",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fati1\\AppData\\Local\\Temp\\ipykernel_9724\\1656931266.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
      "d:\\RAGSummarizer\\attempt1\\myvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ac218",
   "metadata": {},
   "source": [
    "We will be using the semantic Chunker from langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2266552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_chunker = SemanticChunker(embedding_model, breakpoint_threshold_type=\"percentile\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "662cc2a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chunk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m chunks = semantic_chunker.split_text(text)\n\u001b[32m      3\u001b[39m metadatas = [\n\u001b[32m      4\u001b[39m     {\n\u001b[32m      5\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: file_path,\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m\"\u001b[39m: i,\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(\u001b[43mchunk\u001b[49m)\n\u001b[32m      8\u001b[39m \n\u001b[32m      9\u001b[39m     }\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(chunks))\n\u001b[32m     11\u001b[39m ]\n",
      "\u001b[31mNameError\u001b[39m: name 'chunk' is not defined"
     ]
    }
   ],
   "source": [
    "chunks = semantic_chunker.split_text(text)\n",
    "\n",
    "metadatas = [\n",
    "    {\n",
    "        \"source\": file_path,\n",
    "        \"chunk_index\": i,\n",
    "        \"length\": len(chunk)\n",
    "\n",
    "    }\n",
    "    for i in range(len(chunks))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a234ff5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 6\n",
      "\n",
      "Chunk 1 length: 1208 characters\n",
      "Chunk 1:\n",
      "Project Title: Document Summarization using Retrieval-Augmented. Generation (RAG). Objective:\n",
      "To develop a summarization system that combines retrieval-based context. selection with large language model (LLM) generation. The system should. accept a long document and generate a concise, coherent summary using. semantic chunking and RAG. Project Tasks:\n",
      "1. Document Ingestion. Accept documents in PDF, TXT, or Markdown format. Split into semantically meaningful chunks using sliding windows or. semantic segmenters. 2. Embedding & Retrieval. Convert chunks to vector embeddings using SentenceTransformers or. OpenAI API. Store in FAISS or Chroma vector DB. Perform semantic retrieval for a general summary query (e.g.,. \"Summarize this document\"). 3. Summary Generation. Use top-k retrieved chunks and pass them into a pre-trained LLM. (e.g., GPT, LLaMA, Mistral). Generate a final summary that is coherent, fluent, and accurate. 4. Output Presentation. Display the retrieved context and the generated summary. . Optionally show token usage, latency, and similarity scores. Dataset Suggestions:\n",
      "Dataset. ArXiv Abstracts. CNN/DailyMail. Description. Scientific article summaries. News article + summary. pairs.\n",
      "\n",
      "Chunk 2 length: 331 characters\n",
      "Chunk 2:\n",
      "Link. Custom PDFs Local/public documents. for summarization. Use your own documents or publicly available. datasets. Tutorials & Guides. HuggingFace RAG Tutorial:\n",
      " LangChain Quickstart:\n",
      " FAISS Vector Store Tutorial:\n",
      "Research Papers & Articles. Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. (Lewis et al., 2020).\n",
      "\n",
      "Chunk 3 length: 341 characters\n",
      "Chunk 3:\n",
      "Don't Pay Attention: Pay Enough! RAG revisited (Borgeaud et al.,. 2022) ColBERT: Efficient and Effective Passage Retrieval. Tools & Libraries. LangChain:\n",
      " FAISS:\n",
      " ChromaDB:\n",
      " SentenceTransformers:\n",
      "Note to Interns:\n",
      "This is a simplified project focusing on the fundamentals of. summarization using pre-trained models and vector search. You are.\n",
      "\n",
      "Chunk 4 length: 197 characters\n",
      "Chunk 4:\n",
      "encouraged to be creative and modular in design. We will evaluate your. effort and understanding, not just the final output. Submission: ZIP file containing code, a PDF report, and sample. results.\n",
      "\n",
      "Chunk 5 length: 154 characters\n",
      "Chunk 5:\n",
      "Grading Rubric (100 Points):\n",
      "Category. Document Parsing. Embedding & Storage. Retrieval Quality. Summary Generation. Pipeline Design. Output Presentation.\n",
      "\n",
      "Chunk 6 length: 612 characters\n",
      "Chunk 6:\n",
      "Documentation. Max Points. 15. 15. 20. 20. 10. 10. 10. Evaluation Criteria. Clean loading, chunking, and formatting. Efficient use of vector DB and embeddings. Relevance of selected content to the document's core idea. Fluency, coverage, and accuracy of the summary. Clear, modular, and reproducible code. Display of retrieved content and generated results. ReadMe clarity, report explanation,and visual aids. Submission Requirements:\n",
      " Python code with requirements.txt or environment.yml. ReadMe with setup and usage guide. Sample summarization runs for at least 3 different documents. PDF report (2 pages max).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of chunks: {len(chunks)}\\n\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1} length: {len(chunk)} characters\")\n",
    "    print(f\"Chunk {i+1}:\\n{chunk}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f74c726",
   "metadata": {},
   "source": [
    "## Vector Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d69730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fati1\\AppData\\Local\\Temp\\ipykernel_21688\\2075795383.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "db = Chroma(\n",
    "    persist_directory=\"chroma_store\",\n",
    "    embedding_function=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4870864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e43d4365-5997-4c86-a174-9f6c76d5ce64',\n",
       " '34621035-0ae9-495e-be6f-718cc4f5630d',\n",
       " '0a684e0c-e222-4092-bc55-63212e203e39',\n",
       " '3f8217eb-25bc-4cc3-aac6-71394330b725',\n",
       " '324191a0-0e34-41a4-90e9-0365d4888310',\n",
       " 'a9b5d231-49dc-4dff-a428-39b89b26812f']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.add_texts( texts=chunks, metadatas=metadatas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e15fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fati1\\AppData\\Local\\Temp\\ipykernel_21688\\123899826.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "db.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7afaad",
   "metadata": {},
   "source": [
    "## Query "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa0603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
