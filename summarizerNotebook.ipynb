{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "143bfbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fati1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdfplumber\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import markdown\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c211c12",
   "metadata": {},
   "source": [
    "## Document Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75c5a9c",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b626fa",
   "metadata": {},
   "source": [
    "File Path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76203ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './problemStatement.md'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639a0f4b",
   "metadata": {},
   "source": [
    "Get the file extension and parse it accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7922e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get file extension\n",
    "def get_file_extension(file_path):\n",
    "    return os.path.splitext(file_path)[-1].lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80804ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the appropriate text parser based on file extension (pdf, markdown, txt)\n",
    "def parse_file(file_path):\n",
    "    file_extension = get_file_extension(file_path)\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = ''\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text() + '\\n'\n",
    "            return text\n",
    "    elif file_extension == '.md' or file_extension == '.markdown':\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            html = markdown.markdown(file.read())\n",
    "        return BeautifulSoup(html, \"html.parser\").get_text()\n",
    "    elif file_extension == '.txt':\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {file_extension}\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b58f1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = parse_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec419c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Title: Document Summarization using Retrieval-Augmented\n",
      "Generation (RAG)\n",
      "Objective:\n",
      "To develop a summarization system that combines retrieval-based context\n",
      "selection with large language model (LLM) generation. The system should\n",
      "accept a long document and generate a concise, coherent summary using\n",
      "semantic chunking and RAG.\n",
      "\n",
      "Project Tasks:\n",
      "1. Document Ingestion\n",
      "● Accept documents in PDF, TXT, or Markdown format.\n",
      "● Split into semantically meaningful chunks using sliding windows or\n",
      "semantic segmenters.\n",
      "2. Embedding & Retrieval\n",
      "● Convert chunks to vector embeddings using SentenceTransformers or\n",
      "OpenAI API. ● Store in FAISS or Chroma vector DB.\n",
      "● Perform semantic retrieval for a general summary query (e.g.,\n",
      "\"Summarize this document\").\n",
      "3. Summary Generation\n",
      "● Use top-k retrieved chunks and pass them into a pre-trained LLM\n",
      "(e.g., GPT, LLaMA, Mistral).\n",
      "● Generate a final summary that is coherent, fluent, and accurate.\n",
      "4. Output Presentation\n",
      "● Display the retrieved context and the generated summary. ●\n",
      "Optionally show token usage, latency, and similarity scores.\n",
      "Dataset Suggestions:\n",
      "Dataset\n",
      "ArXiv Abstracts\n",
      "CNN/DailyMail\n",
      "Description\n",
      "\n",
      "Scientific article summaries\n",
      "News article + summary\n",
      "pairs\n",
      "\n",
      "Link\n",
      "\n",
      "https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
      "https://huggingface.co/datasets/cnn_dailymail\n",
      "\n",
      "Custom PDFs Local/public documents\n",
      "for summarization\n",
      "\n",
      "Use your own documents or publicly available\n",
      "datasets\n",
      "\n",
      "Tutorials & Guides\n",
      "● HuggingFace RAG Tutorial:\n",
      "https://huggingface.co/blog/rag\n",
      "● LangChain Quickstart:\n",
      "https://docs.langchain.com/docs/get_started/introduction\n",
      "● FAISS Vector Store Tutorial:\n",
      "https://github.com/facebookresearch/faiss/wiki/Getting-started\n",
      "Research Papers & Articles\n",
      "● Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "(Lewis et al., 2020)\n",
      "● Don't Pay Attention: Pay Enough! RAG revisited (Borgeaud et al.,\n",
      "2022) ● ColBERT: Efficient and Effective Passage Retrieval\n",
      "Tools & Libraries\n",
      "● LangChain:\n",
      "https://github.com/hwchase17/langchain\n",
      "● FAISS:\n",
      "https://github.com/facebookresearch/faiss\n",
      "● ChromaDB:\n",
      "https://www.trychroma.com/\n",
      "● SentenceTransformers:\n",
      "https://www.sbert.net/\n",
      "\n",
      "Note to Interns:\n",
      "This is a simplified project focusing on the fundamentals of\n",
      "summarization using pre-trained models and vector search. You are\n",
      "encouraged to be creative and modular in design. We will evaluate your\n",
      "effort and understanding, not just the final output.\n",
      "\n",
      "Submission: ZIP file containing code, a PDF report, and sample\n",
      "results\n",
      "Grading Rubric (100 Points):\n",
      "Category\n",
      "\n",
      "Document Parsing\n",
      "Embedding & Storage\n",
      "Retrieval Quality\n",
      "Summary Generation\n",
      "Pipeline Design\n",
      "Output Presentation\n",
      "Documentation\n",
      "\n",
      "Max Points\n",
      "\n",
      "15\n",
      "15\n",
      "20\n",
      "20\n",
      "10\n",
      "10\n",
      "10\n",
      "\n",
      "Evaluation Criteria\n",
      "\n",
      "Clean loading, chunking, and formatting\n",
      "Efficient use of vector DB and embeddings\n",
      "Relevance of selected content to the document's core idea\n",
      "Fluency, coverage, and accuracy of the summary\n",
      "Clear, modular, and reproducible code\n",
      "Display of retrieved content and generated results\n",
      "ReadMe clarity, report explanation,and visual aids\n",
      "\n",
      "Submission Requirements:\n",
      "● Python code with requirements.txt or environment.yml\n",
      "● ReadMe with setup and usage guide\n",
      "● Sample summarization runs for at least 3 different documents\n",
      "● PDF report (2 pages max)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aded9262",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b720e64",
   "metadata": {},
   "source": [
    "Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4786e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Normalize line breaks and spaces\n",
    "    text = re.sub(r'\\r\\n|\\r', '\\n', text)           # Convert \\r\\n or \\r to \\n\n",
    "    text = re.sub(r'\\n{2,}', '\\n\\n', text)          # Collapse many newlines into 2\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)             # Remove extra spaces/tabs\n",
    "\n",
    "    # Normalize unicode \n",
    "    text = unicodedata.normalize(\"NFKD\", text)\n",
    "\n",
    "    # Remove common bullet points\n",
    "    text = re.sub(\n",
    "        r'[\\u2022\\u2023\\u25E6\\u2043\\u2219\\u25AA\\u25AB\\u25CB\\u25CF\\u25A0\\u25B8\\u29BE\\u29BF]',\n",
    "          '', text)\n",
    "\n",
    "    # Remove markdown or ASCII-style tables\n",
    "    text = re.sub(r'\\|.*?\\|', '', text)      # Remove markdown tables\n",
    "    text = re.sub(r'[-=]{3,}', '', text)     # Remove underlines in tables\n",
    "    text = re.sub(r'^\\s*[\\-\\*+]\\s+', '', text, flags=re.MULTILINE)  # Bulleted list lines\n",
    "\n",
    "    # Remove figure/table/image captions\n",
    "    text = re.sub(r'(Figure|Table|Image|Chart|Diagram)\\s*\\d+[\\.:]?', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove bracketed footnotes like [1], [12], (Fig. 3), etc.\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?fig.*?\\)', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Fix line breaks and hyphens split across lines\n",
    "    text = re.sub(r'-\\n', '', text)  # Remove hyphenated line-breaks\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Collapse newlines\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)  # Normalize spaces\n",
    "\n",
    "    # Strip remaining non-ASCII or odd symbols\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a429201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project Title: Document Summarization using Retrieval-Augmented\n",
      "Generation (RAG)\n",
      "Objective:\n",
      "To develop a summarization system that combines retrieval-based context\n",
      "selection with large language model (LLM) generation. The system should\n",
      "accept a long document and generate a concise, coherent summary using\n",
      "semantic chunking and RAG.\n",
      "Project Tasks:\n",
      "1. Document Ingestion\n",
      " Accept documents in PDF, TXT, or Markdown format.\n",
      " Split into semantically meaningful chunks using sliding windows or\n",
      "semantic segmenters.\n",
      "2. Embedding & Retrieval\n",
      " Convert chunks to vector embeddings using SentenceTransformers or\n",
      "OpenAI API. Store in FAISS or Chroma vector DB.\n",
      " Perform semantic retrieval for a general summary query (e.g.,\n",
      "\"Summarize this document\").\n",
      "3. Summary Generation\n",
      " Use top-k retrieved chunks and pass them into a pre-trained LLM\n",
      "(e.g., GPT, LLaMA, Mistral).\n",
      " Generate a final summary that is coherent, fluent, and accurate.\n",
      "4. Output Presentation\n",
      " Display the retrieved context and the generated summary. \n",
      "Optionally show token usage, latency, and similarity scores.\n",
      "Dataset Suggestions:\n",
      "Dataset\n",
      "ArXiv Abstracts\n",
      "CNN/DailyMail\n",
      "Description\n",
      "Scientific article summaries\n",
      "News article + summary\n",
      "pairs\n",
      "Link\n",
      "Custom PDFs Local/public documents\n",
      "for summarization\n",
      "Use your own documents or publicly available\n",
      "datasets\n",
      "Tutorials & Guides\n",
      " HuggingFace RAG Tutorial:\n",
      " LangChain Quickstart:\n",
      " FAISS Vector Store Tutorial:\n",
      "Research Papers & Articles\n",
      " Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n",
      "(Lewis et al., 2020)\n",
      " Don't Pay Attention: Pay Enough! RAG revisited (Borgeaud et al.,\n",
      "2022) ColBERT: Efficient and Effective Passage Retrieval\n",
      "Tools & Libraries\n",
      " LangChain:\n",
      " FAISS:\n",
      " ChromaDB:\n",
      " SentenceTransformers:\n",
      "Note to Interns:\n",
      "This is a simplified project focusing on the fundamentals of\n",
      "summarization using pre-trained models and vector search. You are\n",
      "encouraged to be creative and modular in design. We will evaluate your\n",
      "effort and understanding, not just the final output.\n",
      "Submission: ZIP file containing code, a PDF report, and sample\n",
      "results\n",
      "Grading Rubric (100 Points):\n",
      "Category\n",
      "Document Parsing\n",
      "Embedding & Storage\n",
      "Retrieval Quality\n",
      "Summary Generation\n",
      "Pipeline Design\n",
      "Output Presentation\n",
      "Documentation\n",
      "Max Points\n",
      "15\n",
      "15\n",
      "20\n",
      "20\n",
      "10\n",
      "10\n",
      "10\n",
      "Evaluation Criteria\n",
      "Clean loading, chunking, and formatting\n",
      "Efficient use of vector DB and embeddings\n",
      "Relevance of selected content to the document's core idea\n",
      "Fluency, coverage, and accuracy of the summary\n",
      "Clear, modular, and reproducible code\n",
      "Display of retrieved content and generated results\n",
      "ReadMe clarity, report explanation,and visual aids\n",
      "Submission Requirements:\n",
      " Python code with requirements.txt or environment.yml\n",
      " ReadMe with setup and usage guide\n",
      " Sample summarization runs for at least 3 different documents\n",
      " PDF report (2 pages max)\n"
     ]
    }
   ],
   "source": [
    "text = clean_text(raw_text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058adda6",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea0e617",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
